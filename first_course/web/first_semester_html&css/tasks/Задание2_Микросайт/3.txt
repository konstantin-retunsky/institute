3. Поисковые роботы

За последние годы Всемирная паутина стала настолько популярной, что сейчас Интернет является одним из основных средств публикации информации. Когда размер Сети вырос из нескольких серверов и небольшого числа документов до огромных пределов, стало ясно, что ручная навигация по значительной части структуры гипертекстовых ссылок больше не представляется возможной, не говоря уже об эффективном методе исследования ресурсов. 

Эта проблема побудила исследователей Интернет на проведение экспериментов с автоматизированной навигацией по Сети, названной "роботами". Веб-робот - это программа, которая перемещается по гипертекстовой структуре Сети, запрашивает документ и рекурсивно возвращает все документы, на которые данный документ ссылается. Эти программы также иногда называют "пауками", " странниками", или " червями" и эти названия, возможно, более привлекательны, однако, могут ввести в заблуждение, поскольку термин "паук" и "странник" cоздает ложное представление, что робот сам перемещается, а термин "червь" мог бы подразумевать, что робот еще и размножается подобно интернетовскому вирусу-червю. В действительности, роботы реализованы как простая программная система, которая запрашивает информацию из удаленных участков Интернет, используя стандартные cетевые протоколы. 

3.1 Использование поисковых роботов 

Роботы могут использоваться для выполнения множества полезных задач, таких как статистический анализ, обслуживание гипертекстов, исследования ресурсов или зазеркаливания страниц. 

3.2 Повышение затрат и потенциальные опасности при использовании поисковых роботов 

Использование роботов может дорого обойтись, особенно в случае, когда они используются удаленно в Интернете. В этом разделе мы увидим, что роботы могут быть опасны, так как они предъявляют слишком высокие требования к Сети. 

3.3 Роботы / агенты клиента 

Загрузка Сети является особой проблемой, связанной с применением категории роботов, которые используются конечными пользователями и реализованы как часть веб-клиента общего назначения (например, Fish Search и tkWWW робот). 

Опасный аспект использования клиентского робота заключается в том, что как только он был распространен по Сети, никакие ошибки уже не могут быть исправлены, не могут быть добавлены никакие знания проблемных областей и никакие новые эффективные свойства не могут его улучшить, как не каждый пользователь впоследствии будет модернизировать этого робота самой последней версией. 

Наиболее опасный аспект, однако - большое количество возможных пользователей роботов. Некоторые люди, вероятно, будут использовать такое устройство здраво, то есть ограничиваться некоторым максимумом ссылок в известной области Сети и в течение короткого периода времени, но найдутся и люди, которые злоупотребят им из-за невежества или высокомерия. По мнению автора, удаленные роботы не должны передаваться конечным пользователям, и к счастью, до сих пор удавалось убедить по крайней мере некоторых авторов роботов не распространять их открыто. 
